{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObGlo7i1BpXk10aq3yFX0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojakabber7/DallE2-ai-generator-scott/blob/main/Non%20DRL%20Recommenders/Dataset1_RetailRocket/SNQNRRitemfeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44Vts3gCKJcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6dca7a-1915-4616-b524-1ec93387f75a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -R /content/Deep_Reinforcement_Learning_Recommenders\n",
        "!git clone https://github.com/kashafali8/Deep_Reinforcement_Learning_Recommenders.git"
      ],
      "metadata": {
        "id": "3D6qmTwmKe4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77eae71-1e5e-4829-f898-43b657c92545"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/Deep_Reinforcement_Learning_Recommenders': No such file or directory\n",
            "Cloning into 'Deep_Reinforcement_Learning_Recommenders'...\n",
            "remote: Enumerating objects: 580, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 580 (delta 44), reused 21 (delta 9), pack-reused 506\u001b[K\n",
            "Receiving objects: 100% (580/580), 8.65 MiB | 10.92 MiB/s, done.\n",
            "Resolving deltas: 100% (340/340), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trfl\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "iG99cgGuKfzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712aa7a9-5ce9-4d5d-ad67-64614d8be326"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trfl\n",
            "  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from trfl) (1.14.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from trfl) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trfl) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trfl) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trfl) (1.4.0)\n",
            "Installing collected packages: trfl\n",
            "Successfully installed trfl-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle -q      # At first, I suspect the kaggle API lose effect so it doesn't have .kaggle folder. (not working)\n",
        "!rm -rf /root/.kaggle.      # when I created the folder, it says the file or dir already exits\n",
        "!mkdir /root/.kaggle        # successful\n",
        "!mv /content/kaggle.json /root/.kaggle/kaggle.json    # not sure if I have to use full destination path, I previously only used /root/.kaggle and it failed. Don't have time to validate this thought.\n",
        "!ls /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFT9XEAbSuZX",
        "outputId": "25e617ff-d90e-462b-fbc5-89e1754e636f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.kaggle/kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d retailrocket/ecommerce-dataset\n",
        "!unzip ecommerce-dataset.zip -d \"/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data/\"\n",
        "!rm ecommerce-dataset.zip"
      ],
      "metadata": {
        "id": "rxSfn8xYKfrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4ed363-8a27-4a56-bf2f-1cddbc6c478e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading ecommerce-dataset.zip to /content\n",
            "100% 290M/291M [00:13<00:00, 24.2MB/s]\n",
            "100% 291M/291M [00:13<00:00, 22.2MB/s]\n",
            "Archive:  ecommerce-dataset.zip\n",
            "  inflating: /content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data/category_tree.csv  \n",
            "  inflating: /content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data/events.csv  \n",
            "  inflating: /content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data/item_properties_part1.csv  \n",
            "  inflating: /content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data/item_properties_part2.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/replay_buffer.py\" --data \"/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data/\""
      ],
      "metadata": {
        "id": "L0cu-rPrKfpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2547976c-a9bc-44db-e045-e0891396d6e9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting to pre-process data...\n",
            "\n",
            "Sorting and pickling data...\n",
            "\n",
            "Splitting data into train, validation, and test sets...\n",
            "\n",
            "Pickling train, validation, and test sets...\n",
            "\n",
            "Calculating item popularity and storing as dictionary...\n",
            "\n",
            "Generating replay buffer from train set...\n",
            "\n",
            "Pickling replay buffer...\n",
            "\n",
            "Pickling data statistics...\n",
            "\n",
            "Script completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_feature_matrix(\n",
        "    sorted_events, n_files=2, path_name=\"\", one_hot_encode=True, top_features=500\n",
        "):\n",
        "    for i in range(n_files):\n",
        "        if i == 0:\n",
        "            item_features = pd.read_csv(path_name + str(i + 1) + \".csv\")\n",
        "        else:\n",
        "            item_features = pd.concat(\n",
        "                [item_features, pd.read_csv(path_name + str(i + 1) + \".csv\")],\n",
        "                ignore_index=True,\n",
        "            )\n",
        "\n",
        "    item_features = item_features[\n",
        "        item_features[\"itemid\"].isin(sorted_events[\"item_id\"].unique().tolist())\n",
        "    ].drop_duplicates()\n",
        "    item_features[\"property_value\"] = (\n",
        "        item_features[\"property\"].str.strip() + item_features[\"value\"].str.strip()\n",
        "    )\n",
        "    item_features = item_features.drop([\"timestamp\"], axis=1).drop_duplicates()\n",
        "\n",
        "    if one_hot_encode:\n",
        "        one_hot_encoded = pd.DataFrame()\n",
        "        itemids = []\n",
        "\n",
        "        event_item_list = sorted_events.item_id.unique()\n",
        "        event_item_list.sort()\n",
        "        item_list = item_features[\"itemid\"].unique()\n",
        "        properties = (\n",
        "            item_features[\"property_value\"]\n",
        "            .value_counts()\n",
        "            .head(top_features)\n",
        "            .index.tolist()\n",
        "        )\n",
        "\n",
        "        for item in event_item_list:\n",
        "            if len(itemids) % 1000 == 0:\n",
        "                print(\"hi\")\n",
        "            if item not in item_list:\n",
        "                one_hot_encoded = pd.concat(\n",
        "                    [one_hot_encoded, pd.DataFrame(np.zeros(len(properties))).T],\n",
        "                    ignore_index=True,\n",
        "                )\n",
        "                itemids.append(item)\n",
        "                continue\n",
        "\n",
        "            item_properties = item_features[item_features[\"itemid\"] == item][\n",
        "                \"property_value\"\n",
        "            ].unique()\n",
        "            one_hot_encoded = pd.concat(\n",
        "                [\n",
        "                    one_hot_encoded,\n",
        "                    pd.DataFrame(\n",
        "                        [1 if x in item_properties else 0 for x in properties]\n",
        "                    ).T,\n",
        "                ],\n",
        "                ignore_index=True,\n",
        "            )\n",
        "            itemids.append(item)\n",
        "\n",
        "        return one_hot_encoded, itemids\n",
        "\n",
        "    else:\n",
        "        return item_features, item_features[\"itemid\"].unique().tolist()"
      ],
      "metadata": {
        "id": "1Eeml5sWK2tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_events = pd.read_pickle(\"/content/Deep_Reinforcement_Learning_Recommenders/DRL_Recommenders/Dataset_RR/data/sorted_events.df\")\n",
        "one_hot_encoded, itemids = create_feature_matrix(\n",
        "    sorted_events, n_files=2, path_name=\"/content/Deep_Reinforcement_Learning_Recommenders/DRL_Recommenders/Dataset_RR/data/item_properties_part\", one_hot_encode=True\n",
        ")"
      ],
      "metadata": {
        "id": "urPNTwGAK2px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded.to_pickle(\"/content/Deep_Reinforcement_Learning_Recommenders/DRL_Recommenders/Dataset_RR/data/one_hot_encoded.df\")"
      ],
      "metadata": {
        "id": "ekyGe9XbK2mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py\" --model=SASRec --epoch=5 --data=\"/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/data\""
      ],
      "metadata": {
        "id": "wHmCUZEOK2jh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588b0f77-c175-40d7-8883-d019094faa6e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-05 00:42:44.627274: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-05 00:42:45.591603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:185: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.seq = tf.compat.v1.layers.dropout(self.seq,\n",
            "2023-05-05 00:42:49.115270: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-05 00:42:49.182177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:42:49.642083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:42:49.642485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:142: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:143: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:144: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:184: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "2023-05-05 00:42:49.866370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:223: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:224: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "2023-05-05 00:42:49.946517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:228: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:229: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "2023-05-05 00:42:49.998734: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:213: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:228: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.w_f = tf.compat.v1.layers.dense(\n",
            "w_f SHAPE:  (70852, 64)\n",
            "phi 2 SHAPE:  (None, 70852)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:185: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.seq = tf.compat.v1.layers.dropout(self.seq,\n",
            "2023-05-05 00:42:53.049775: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:142: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:143: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:144: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:184: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "2023-05-05 00:42:53.195965: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:223: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:224: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "2023-05-05 00:42:53.249460: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:228: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SASRecModules.py:229: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "2023-05-05 00:42:53.291712: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:213: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Deep_Reinforcement_Learning_Recommenders/Non DRL Recommenders/Dataset1_RetailRocket/src/SNQN_IF.py:228: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.w_f = tf.compat.v1.layers.dense(\n",
            "w_f SHAPE:  (70852, 64)\n",
            "phi 2 SHAPE:  (None, 70852)\n",
            "2023-05-05 00:42:57.810428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:42:57.810932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:42:57.811169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:43:00.193674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:43:00.193949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:43:00.194144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-05 00:43:00.194288: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-05 00:43:00.194332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13678 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-05-05 00:43:01.340891: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "2023-05-05 00:43:07.996409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
            "the loss in 200th batch is: 10.834751\n",
            "the loss in 400th batch is: 10.140614\n",
            "the loss in 600th batch is: 10.007078\n",
            "the loss in 800th batch is: 10.050577\n",
            "the loss in 1000th batch is: 9.578543\n",
            "the loss in 1200th batch is: 9.548000\n",
            "the loss in 1400th batch is: 9.401826\n",
            "the loss in 1600th batch is: 8.940021\n",
            "the loss in 1800th batch is: 8.830302\n",
            "the loss in 2000th batch is: 8.843123\n",
            "the loss in 2200th batch is: 8.537320\n",
            "the loss in 2400th batch is: 8.532778\n",
            "the loss in 2600th batch is: 8.534201\n",
            "the loss in 2800th batch is: 7.920466\n",
            "the loss in 3000th batch is: 8.377593\n",
            "the loss in 3200th batch is: 7.864102\n",
            "the loss in 3400th batch is: 7.548781\n",
            "the loss in 3600th batch is: 7.724858\n",
            "the loss in 3800th batch is: 7.811736\n",
            "the loss in 4000th batch is: 7.483284\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 0.800000\n",
            "clicks hr ndcg @ 5 : 0.000034, 0.000020\n",
            "purchase hr and ndcg @5 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 3.200000\n",
            "clicks hr ndcg @ 10 : 0.000137, 0.000053\n",
            "purchase hr and ndcg @10 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5.200000\n",
            "clicks hr ndcg @ 15 : 0.000222, 0.000075\n",
            "purchase hr and ndcg @15 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 8.400000\n",
            "clicks hr ndcg @ 20 : 0.000316, 0.000097\n",
            "purchase hr and ndcg @20 : 0.000179, 0.000042\n",
            "#############################################################\n",
            "the loss in 4200th batch is: 7.436753\n",
            "the loss in 4400th batch is: 7.503499\n",
            "the loss in 4600th batch is: 6.998168\n",
            "the loss in 4800th batch is: 7.724980\n",
            "the loss in 5000th batch is: 6.915792\n",
            "the loss in 5200th batch is: 7.068910\n",
            "the loss in 5400th batch is: 7.148920\n",
            "the loss in 5600th batch is: 6.808349\n",
            "the loss in 5800th batch is: 7.101782\n",
            "the loss in 6000th batch is: 7.046910\n",
            "the loss in 6200th batch is: 7.284247\n",
            "the loss in 6400th batch is: 6.829310\n",
            "the loss in 6600th batch is: 6.742812\n",
            "the loss in 6800th batch is: 6.724401\n",
            "the loss in 7000th batch is: 6.407224\n",
            "the loss in 7200th batch is: 6.330208\n",
            "the loss in 7400th batch is: 6.070740\n",
            "the loss in 7600th batch is: 6.463719\n",
            "the loss in 7800th batch is: 6.339008\n",
            "the loss in 8000th batch is: 6.014837\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 1.000000\n",
            "clicks hr ndcg @ 5 : 0.000043, 0.000021\n",
            "purchase hr and ndcg @5 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 2.800000\n",
            "clicks hr ndcg @ 10 : 0.000120, 0.000045\n",
            "purchase hr and ndcg @10 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 4.200000\n",
            "clicks hr ndcg @ 15 : 0.000137, 0.000049\n",
            "purchase hr and ndcg @15 : 0.000179, 0.000047\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6.200000\n",
            "clicks hr ndcg @ 20 : 0.000222, 0.000070\n",
            "purchase hr and ndcg @20 : 0.000179, 0.000047\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 5.733220\n",
            "the loss in 8400th batch is: 6.067472\n",
            "the loss in 8600th batch is: 6.610891\n",
            "the loss in 8800th batch is: 6.120895\n",
            "the loss in 9000th batch is: 5.695564\n",
            "the loss in 9200th batch is: 6.216830\n",
            "the loss in 9400th batch is: 5.584511\n",
            "the loss in 9600th batch is: 6.598884\n",
            "the loss in 9800th batch is: 5.371517\n",
            "the loss in 10000th batch is: 5.858021\n",
            "the loss in 10200th batch is: 6.130340\n",
            "the loss in 10400th batch is: 5.853984\n",
            "the loss in 10600th batch is: 5.357780\n",
            "the loss in 10800th batch is: 5.611729\n",
            "the loss in 11000th batch is: 6.174156\n",
            "the loss in 11200th batch is: 5.744211\n",
            "the loss in 11400th batch is: 5.785812\n",
            "the loss in 11600th batch is: 6.077030\n",
            "the loss in 11800th batch is: 5.934965\n",
            "the loss in 12000th batch is: 5.657094\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 2.400000\n",
            "clicks hr ndcg @ 5 : 0.000060, 0.000028\n",
            "purchase hr and ndcg @5 : 0.000179, 0.000069\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4.200000\n",
            "clicks hr ndcg @ 10 : 0.000137, 0.000053\n",
            "purchase hr and ndcg @10 : 0.000179, 0.000069\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 7.400000\n",
            "clicks hr ndcg @ 15 : 0.000231, 0.000077\n",
            "purchase hr and ndcg @15 : 0.000359, 0.000115\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10.000000\n",
            "clicks hr ndcg @ 20 : 0.000299, 0.000093\n",
            "purchase hr and ndcg @20 : 0.000538, 0.000157\n",
            "#############################################################\n",
            "the loss in 12200th batch is: 5.629550\n",
            "the loss in 12400th batch is: 6.289094\n",
            "the loss in 12600th batch is: 5.510935\n",
            "the loss in 12800th batch is: 6.203146\n",
            "the loss in 13000th batch is: 6.094222\n",
            "the loss in 13200th batch is: 5.473129\n",
            "the loss in 13400th batch is: 5.203094\n",
            "the loss in 13600th batch is: 5.221659\n",
            "the loss in 13800th batch is: 5.562335\n",
            "the loss in 14000th batch is: 5.740839\n",
            "the loss in 14200th batch is: 5.395661\n",
            "the loss in 14400th batch is: 5.450089\n",
            "the loss in 14600th batch is: 5.802747\n",
            "the loss in 14800th batch is: 6.217758\n",
            "the loss in 15000th batch is: 5.832220\n",
            "the loss in 15200th batch is: 5.440260\n",
            "the loss in 15400th batch is: 5.805127\n",
            "the loss in 15600th batch is: 5.793663\n",
            "the loss in 15800th batch is: 5.113127\n",
            "the loss in 16000th batch is: 5.624095\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 0.400000\n",
            "clicks hr ndcg @ 5 : 0.000017, 0.000007\n",
            "purchase hr and ndcg @5 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 1.800000\n",
            "clicks hr ndcg @ 10 : 0.000077, 0.000025\n",
            "purchase hr and ndcg @10 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 3.600000\n",
            "clicks hr ndcg @ 15 : 0.000111, 0.000034\n",
            "purchase hr and ndcg @15 : 0.000179, 0.000050\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 5.400000\n",
            "clicks hr ndcg @ 20 : 0.000188, 0.000052\n",
            "purchase hr and ndcg @20 : 0.000179, 0.000050\n",
            "#############################################################\n",
            "the loss in 16200th batch is: 4.989425\n",
            "the loss in 16400th batch is: 5.722969\n",
            "the loss in 16600th batch is: 5.579459\n",
            "the loss in 16800th batch is: 4.812704\n",
            "the loss in 17000th batch is: 5.393141\n",
            "the loss in 17200th batch is: 5.471298\n",
            "the loss in 17400th batch is: 5.342669\n",
            "the loss in 17600th batch is: 5.266124\n",
            "the loss in 17800th batch is: 5.148722\n",
            "the loss in 18000th batch is: 5.348114\n",
            "the loss in 18200th batch is: 5.529724\n",
            "the loss in 18400th batch is: 5.224638\n",
            "the loss in 18600th batch is: 5.531170\n",
            "the loss in 18800th batch is: 5.494450\n",
            "the loss in 19000th batch is: 5.507286\n",
            "the loss in 19200th batch is: 5.508691\n"
          ]
        }
      ]
    }
  ]
}